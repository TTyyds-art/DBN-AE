{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huzuntao/PycharmProjects/DBN-AE/venv/lib/python3.8/site-packages/numba/core/errors.py:175: UserWarning: Insufficiently recent colorama version found. Numba requires colorama >= 0.3.9\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "from DBN import DBN\n",
    "import os\n",
    "import torch\n",
    "import copy\n",
    "import torchvision\n",
    "from torchvision import datasets,transforms\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from powernets.gridinit import grid_init\n",
    "\n",
    "import matplotlib\n",
    "import pandapower.networks as nw  # for case name\n",
    "import matplotlib.pyplot as plt\n",
    "from prp_utils.dataloader import data_loader\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from nerualnets.structnet import CustomNet, DBN_C\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "case_new = nw.case14()\n",
    "case_new.gen['vm_pu'] = 1.00\n",
    "om, net = grid_init(case_name=case_new)\n",
    "l_p = copy.deepcopy(case_new.load['p_mw'])\n",
    "l_p = torch.as_tensor(np.concatenate([l_p.to_numpy(), np.zeros(11)]), dtype=torch.float32)\n",
    "\n",
    "#\n",
    "num_input, num_output = len(net.load) * 2, len(net.bus) * 2\n",
    "\n",
    "path_current = os.getcwd()\n",
    "path_PRP = path_current.rsplit(\"/\", 2)[0]\n",
    "# deal with the data\n",
    "path_add_file = None\n",
    "# _path_4 = '/home/huzuntao/PycharmProjects/Auto_model_Ps2Dl_v1/Power_Relia_Proj/data/bus4_64_1st_2022_8_29/15_9_22.npy'\n",
    "_path_4 = path_PRP +'/' + 'data/bus_14_500_1st_2023_3_8/9_10_48.npy'\n",
    "inps, outs = data_loader(the_path=_path_4, in_num=int(num_input / 2), out_num=int(num_output / 2))\n",
    "inps = inps-l_p\n",
    "\n",
    "# inpu_, labs_ = dataset_[:64 * 5]\n",
    "# dataset = TensorDataset(inpu_, labs_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "'/home/huzuntao/PycharmProjects/DBN-AE'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_current = os.getcwd()\n",
    "path_current\n",
    "path_PRP = path_current.rsplit(\"/\", 2)[0]\n",
    "path_PRP"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 2.5637, -3.3291,  5.0206,  0.7690,  0.4290,  2.6368,  1.2962,  0.5763,\n         -0.6968, -2.2375, -0.4072, 12.7000, 19.0000, -3.9000,  1.6000,  7.5000,\n         16.6000,  5.8000,  1.8000,  1.6000,  5.8000,  5.0000]])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inps[:1]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 1.7856, 16.9898, -5.6308,  1.2520, -0.8544,  5.2334,  0.7907, -0.5432,\n         -0.4318,  2.4507,  0.9800, 12.7000, 19.0000, -3.9000,  1.6000,  7.5000,\n         16.6000,  5.8000,  1.8000,  1.6000,  5.8000,  5.0000]])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inps[1:2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/huzuntao/PycharmProjects/DBN-AE/venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_27001/2842952277.py\", line 2, in <module>\n",
      "    torch.unify_type_list()\n",
      "TypeError: unify_type_list(): incompatible function arguments. The following argument types are supported:\n",
      "    1. (arg0: List[c10::Type]) -> c10::Type\n",
      "\n",
      "Invoked with: \n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/huzuntao/PycharmProjects/DBN-AE/venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2057, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/home/huzuntao/PycharmProjects/DBN-AE/venv/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1288, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/home/huzuntao/PycharmProjects/DBN-AE/venv/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1177, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/home/huzuntao/PycharmProjects/DBN-AE/venv/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1030, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/home/huzuntao/PycharmProjects/DBN-AE/venv/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 960, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/home/huzuntao/PycharmProjects/DBN-AE/venv/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 870, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/home/huzuntao/PycharmProjects/DBN-AE/venv/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 704, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/home/huzuntao/PycharmProjects/DBN-AE/venv/lib/python3.8/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/huzuntao/PycharmProjects/DBN-AE/venv/lib/python3.8/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/home/huzuntao/PycharmProjects/DBN-AE/venv/lib/python3.8/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/huzuntao/PycharmProjects/DBN-AE/venv/lib/python3.8/site-packages/stack_data/core.py\", line 677, in included_pieces\n",
      "    scope_pieces = self.scope_pieces\n",
      "  File \"/home/huzuntao/PycharmProjects/DBN-AE/venv/lib/python3.8/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/huzuntao/PycharmProjects/DBN-AE/venv/lib/python3.8/site-packages/stack_data/core.py\", line 614, in scope_pieces\n",
      "    scope_start, scope_end = self.source.line_range(self.scope)\n",
      "  File \"/home/huzuntao/PycharmProjects/DBN-AE/venv/lib/python3.8/site-packages/stack_data/core.py\", line 178, in line_range\n",
      "    return line_range(self.asttext(), node)\n",
      "  File \"/home/huzuntao/PycharmProjects/DBN-AE/venv/lib/python3.8/site-packages/executing/executing.py\", line 428, in asttext\n",
      "    self._asttext = ASTText(self.text, tree=self.tree, filename=self.filename)\n",
      "  File \"/home/huzuntao/PycharmProjects/DBN-AE/venv/lib/python3.8/site-packages/asttokens/asttokens.py\", line 307, in __init__\n",
      "    super(ASTText, self).__init__(source_text, filename)\n",
      "  File \"/home/huzuntao/PycharmProjects/DBN-AE/venv/lib/python3.8/site-packages/asttokens/asttokens.py\", line 44, in __init__\n",
      "    source_text = six.ensure_text(source_text)\n",
      "AttributeError: module 'six' has no attribute 'ensure_text'\n"
     ]
    }
   ],
   "source": [
    "#Loading dataset and show the content\n",
    "torch.unify_type_list()\n",
    "for item in dataset_[:2]:\n",
    "    print(f\"Input: {item[0]}; \\n Output: {item[1]}\")\n",
    "# a = dataset_[0]\n",
    "# a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have have set these hyper parameters although you can experiment with them to find better hyperparameters.\n",
    "nn_shape = [22, 23*23 ,23*23, 30*23, 18*18]\n",
    "dbn_mnist = DBN(visible_units=22 ,\n",
    "                hidden_units=[23*23 ,23*23, 30*23, 18*18] ,\n",
    "                k = 5,\n",
    "                learning_rate = 0.01,\n",
    "                learning_rate_decay = True,\n",
    "                xavier_init = True,\n",
    "                increase_to_cd_k = False,\n",
    "                use_gpu = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Training the 1 st rbm layer\n",
      "RBM fitting: 25it [00:00, 404.73it/s]\n",
      "Epoch:1 ,avg_cost = 9.550758177971133e+30 ,std_cost = 4.512942328856492e+31 ,avg_grad = 1.275900230906151e+18 ,std_grad = 4.96994813844285e+18\n",
      "RBM fitting: 25it [00:00, 566.70it/s]\n",
      "Epoch:2 ,avg_cost = inf ,std_cost = nan ,avg_grad = 1.7721971499053503e+24 ,std_grad = 4.097552779123763e+24\n",
      "RBM fitting: 25it [00:00, 576.96it/s]\n",
      "Epoch:3 ,avg_cost = inf ,std_cost = nan ,avg_grad = 4.930399639160905e+24 ,std_grad = 7.648938971399152e+24\n",
      "RBM fitting: 25it [00:00, 528.56it/s]\n",
      "Epoch:4 ,avg_cost = inf ,std_cost = nan ,avg_grad = 2.2246612630682017e+21 ,std_grad = 7.778659497640827e+21\n",
      "RBM fitting: 25it [00:00, 580.74it/s]\n",
      "Epoch:5 ,avg_cost = 8.730021245301043e+25 ,std_cost = 2.5724085491419623e+24 ,avg_grad = 0.0 ,std_grad = 0.0\n",
      "RBM fitting: 25it [00:00, 530.44it/s]\n",
      "Epoch:6 ,avg_cost = 7.961002310309795e+25 ,std_cost = 1.9545602206661622e+24 ,avg_grad = 0.0 ,std_grad = 0.0\n",
      "RBM fitting: 25it [00:00, 560.73it/s]\n",
      "Epoch:7 ,avg_cost = 7.365437648675036e+25 ,std_cost = 1.549844852642062e+24 ,avg_grad = 0.0 ,std_grad = 0.0\n",
      "RBM fitting: 25it [00:00, 553.02it/s]\n",
      "Epoch:8 ,avg_cost = 6.886481773859223e+25 ,std_cost = 1.2678336233165756e+24 ,avg_grad = 0.0 ,std_grad = 0.0\n",
      "RBM fitting: 25it [00:00, 526.32it/s]\n",
      "Epoch:9 ,avg_cost = 6.490401124974763e+25 ,std_cost = 1.0620732355988665e+24 ,avg_grad = 0.0 ,std_grad = 0.0\n",
      "RBM fitting: 25it [00:00, 559.33it/s]\n",
      "Epoch:10 ,avg_cost = 6.155726920100071e+25 ,std_cost = 9.065333880458912e+23 ,avg_grad = 0.0 ,std_grad = 0.0\n",
      "RBM fitting: 25it [00:00, 568.08it/s]\n",
      "Epoch:11 ,avg_cost = 5.868059169642607e+25 ,std_cost = 7.855679990914123e+23 ,avg_grad = 0.0 ,std_grad = 0.0\n",
      "RBM fitting: 25it [00:00, 552.78it/s]\n",
      "Epoch:12 ,avg_cost = 5.617322261989312e+25 ,std_cost = 6.89314978185023e+23 ,avg_grad = 0.0 ,std_grad = 0.0\n",
      "RBM fitting: 25it [00:00, 562.57it/s]\n",
      "Epoch:13 ,avg_cost = 5.396227427388061e+25 ,std_cost = 6.112304149241687e+23 ,avg_grad = 0.0 ,std_grad = 0.0\n",
      "RBM fitting: 25it [00:00, 473.77it/s]\n",
      "Epoch:14 ,avg_cost = 5.199356395935803e+25 ,std_cost = 5.468488281487251e+23 ,avg_grad = 0.0 ,std_grad = 0.0\n",
      "RBM fitting: 25it [00:00, 587.58it/s]\n",
      "Epoch:15 ,avg_cost = 5.022583092151852e+25 ,std_cost = 4.9302832661465334e+23 ,avg_grad = 0.0 ,std_grad = 0.0\n",
      "RBM fitting: 25it [00:00, 570.42it/s]\n",
      "Epoch:16 ,avg_cost = 4.862705622433613e+25 ,std_cost = 4.474940520781761e+23 ,avg_grad = 0.0 ,std_grad = 0.0\n",
      "RBM fitting: 25it [00:00, 537.38it/s]\n",
      "Epoch:17 ,avg_cost = 4.717191248819766e+25 ,std_cost = 4.085578392261728e+23 ,avg_grad = 0.0 ,std_grad = 0.0\n",
      "RBM fitting: 25it [00:00, 569.38it/s]\n",
      "Epoch:18 ,avg_cost = 4.584008523619194e+25 ,std_cost = 3.749602654300485e+23 ,avg_grad = 0.0 ,std_grad = 0.0\n",
      "RBM fitting: 25it [00:00, 525.76it/s]\n",
      "Epoch:19 ,avg_cost = 4.461509691417513e+25 ,std_cost = 3.45729958293375e+23 ,avg_grad = 0.0 ,std_grad = 0.0\n",
      "RBM fitting: 25it [00:00, 574.30it/s]\n",
      "Epoch:20 ,avg_cost = 4.348337533019499e+25 ,std_cost = 3.2010480067281003e+23 ,avg_grad = 0.0 ,std_grad = 0.0\n",
      "--------------------\n",
      "Training the 2 st rbm layer\n",
      "RBM fitting: 25it [00:00, 114.42it/s]\n",
      "Epoch:1 ,avg_cost = 0.8259615898132324 ,std_cost = 1.202954649925232 ,avg_grad = 15732.1787109375 ,std_grad = 14597.6767578125\n",
      "RBM fitting: 25it [00:00, 147.44it/s]\n",
      "Epoch:2 ,avg_cost = 0.015592599287629128 ,std_cost = 0.0050694383680820465 ,avg_grad = 3387.510009765625 ,std_grad = 484.93194580078125\n",
      "RBM fitting: 25it [00:00, 115.22it/s]\n",
      "Epoch:3 ,avg_cost = 0.007229628972709179 ,std_cost = 0.0012788844760507345 ,avg_grad = 2409.948974609375 ,std_grad = 197.04306030273438\n",
      "RBM fitting: 25it [00:00, 114.40it/s]\n",
      "Epoch:4 ,avg_cost = 0.004940532147884369 ,std_cost = 0.0007492162985727191 ,avg_grad = 2014.545654296875 ,std_grad = 140.27330017089844\n",
      "RBM fitting: 25it [00:00, 128.54it/s]\n",
      "Epoch:5 ,avg_cost = 0.003805711632594466 ,std_cost = 0.0003932541294489056 ,avg_grad = 1788.5205078125 ,std_grad = 87.46472930908203\n",
      "RBM fitting: 25it [00:00, 114.25it/s]\n",
      "Epoch:6 ,avg_cost = 0.0030487605836242437 ,std_cost = 0.00034559087362140417 ,avg_grad = 1602.255615234375 ,std_grad = 88.81608581542969\n",
      "RBM fitting: 25it [00:00, 130.55it/s]\n",
      "Epoch:7 ,avg_cost = 0.0026989716570824385 ,std_cost = 0.00026695977430790663 ,avg_grad = 1516.4534912109375 ,std_grad = 68.66443634033203\n",
      "RBM fitting: 25it [00:00, 118.47it/s]\n",
      "Epoch:8 ,avg_cost = 0.0024133583065122366 ,std_cost = 0.0002559233398642391 ,avg_grad = 1436.7449951171875 ,std_grad = 72.03318786621094\n",
      "RBM fitting: 25it [00:00, 120.22it/s]\n",
      "Epoch:9 ,avg_cost = 0.002248728182166815 ,std_cost = 0.00018477837147656828 ,avg_grad = 1385.774658203125 ,std_grad = 49.344085693359375\n",
      "RBM fitting: 25it [00:00, 119.54it/s]\n",
      "Epoch:10 ,avg_cost = 0.0020127734169363976 ,std_cost = 0.0001547342399135232 ,avg_grad = 1313.637451171875 ,std_grad = 47.40916442871094\n",
      "RBM fitting: 25it [00:00, 111.36it/s]\n",
      "Epoch:11 ,avg_cost = 0.0018462310545146465 ,std_cost = 0.00016940999194048345 ,avg_grad = 1265.10546875 ,std_grad = 54.65484619140625\n",
      "RBM fitting: 25it [00:00, 116.82it/s]\n",
      "Epoch:12 ,avg_cost = 0.0017529588658362627 ,std_cost = 0.00018401395936962217 ,avg_grad = 1231.4539794921875 ,std_grad = 60.88886260986328\n",
      "RBM fitting: 25it [00:00, 121.42it/s]\n",
      "Epoch:13 ,avg_cost = 0.0017122869612649083 ,std_cost = 0.00016950879944488406 ,avg_grad = 1218.012451171875 ,std_grad = 53.369346618652344\n",
      "RBM fitting: 25it [00:00, 132.58it/s]\n",
      "Epoch:14 ,avg_cost = 0.0015407048631459475 ,std_cost = 0.0001353762054350227 ,avg_grad = 1156.4515380859375 ,std_grad = 47.2397346496582\n",
      "RBM fitting: 25it [00:00, 165.10it/s]\n",
      "Epoch:15 ,avg_cost = 0.0015016746474429965 ,std_cost = 0.00013790969387628138 ,avg_grad = 1142.9913330078125 ,std_grad = 50.52920150756836\n",
      "RBM fitting: 25it [00:00, 153.27it/s]\n",
      "Epoch:16 ,avg_cost = 0.0014507935848087072 ,std_cost = 0.00015678646741434932 ,avg_grad = 1125.9437255859375 ,std_grad = 56.842689514160156\n",
      "RBM fitting: 25it [00:00, 156.21it/s]\n",
      "Epoch:17 ,avg_cost = 0.0013865052023902535 ,std_cost = 0.00018288979481440037 ,avg_grad = 1094.1815185546875 ,std_grad = 69.4393539428711\n",
      "RBM fitting: 25it [00:00, 118.75it/s]\n",
      "Epoch:18 ,avg_cost = 0.001353295287117362 ,std_cost = 0.00014303727948572487 ,avg_grad = 1088.0 ,std_grad = 52.83189392089844\n",
      "RBM fitting: 25it [00:00, 126.97it/s]\n",
      "Epoch:19 ,avg_cost = 0.0013017074670642614 ,std_cost = 0.00014338442997541279 ,avg_grad = 1067.5997314453125 ,std_grad = 55.18672561645508\n",
      "RBM fitting: 25it [00:00, 142.17it/s]\n",
      "Epoch:20 ,avg_cost = 0.0012226281687617302 ,std_cost = 0.0001100429508369416 ,avg_grad = 1035.2086181640625 ,std_grad = 43.64979934692383\n",
      "--------------------\n",
      "Training the 3 st rbm layer\n",
      "RBM fitting: 25it [00:00, 131.16it/s]\n",
      "Epoch:1 ,avg_cost = 5.118165969848633 ,std_cost = 0.018287863582372665 ,avg_grad = 29123.79296875 ,std_grad = 751.4484252929688\n",
      "RBM fitting: 25it [00:00, 132.81it/s]\n",
      "Epoch:2 ,avg_cost = 5.084773540496826 ,std_cost = 0.014280052855610847 ,avg_grad = 27416.373046875 ,std_grad = 463.6500244140625\n",
      "RBM fitting: 25it [00:00, 138.53it/s]\n",
      "Epoch:3 ,avg_cost = 5.067111015319824 ,std_cost = 0.011905839666724205 ,avg_grad = 27154.52734375 ,std_grad = 404.06085205078125\n",
      "RBM fitting: 25it [00:00, 141.34it/s]\n",
      "Epoch:4 ,avg_cost = 5.0654988288879395 ,std_cost = 0.010066106915473938 ,avg_grad = 26903.4296875 ,std_grad = 441.250732421875\n",
      "RBM fitting: 25it [00:00, 116.21it/s]\n",
      "Epoch:5 ,avg_cost = 5.063861846923828 ,std_cost = 0.013796697370707989 ,avg_grad = 26937.25 ,std_grad = 361.86553955078125\n",
      "RBM fitting: 25it [00:00, 110.97it/s]\n",
      "Epoch:6 ,avg_cost = 5.061037540435791 ,std_cost = 0.014013627544045448 ,avg_grad = 26681.091796875 ,std_grad = 471.0859375\n",
      "RBM fitting: 25it [00:00, 108.71it/s]\n",
      "Epoch:7 ,avg_cost = 5.0586347579956055 ,std_cost = 0.01101619377732277 ,avg_grad = 26690.734375 ,std_grad = 373.0373840332031\n",
      "RBM fitting: 25it [00:00, 125.00it/s]\n",
      "Epoch:8 ,avg_cost = 5.058076858520508 ,std_cost = 0.010125220753252506 ,avg_grad = 26612.04296875 ,std_grad = 369.6484069824219\n",
      "RBM fitting: 25it [00:00, 123.62it/s]\n",
      "Epoch:9 ,avg_cost = 5.056305408477783 ,std_cost = 0.011388002894818783 ,avg_grad = 26572.552734375 ,std_grad = 460.3002014160156\n",
      "RBM fitting: 25it [00:00, 138.22it/s]\n",
      "Epoch:10 ,avg_cost = 5.055077075958252 ,std_cost = 0.01168521773070097 ,avg_grad = 26620.134765625 ,std_grad = 480.23602294921875\n",
      "RBM fitting: 25it [00:00, 123.12it/s]\n",
      "Epoch:11 ,avg_cost = 5.050240516662598 ,std_cost = 0.012321533635258675 ,avg_grad = 26622.849609375 ,std_grad = 336.5692443847656\n",
      "RBM fitting: 25it [00:00, 135.44it/s]\n",
      "Epoch:12 ,avg_cost = 5.055552005767822 ,std_cost = 0.012066434137523174 ,avg_grad = 26654.4765625 ,std_grad = 365.7872009277344\n",
      "RBM fitting: 25it [00:00, 133.35it/s]\n",
      "Epoch:13 ,avg_cost = 5.050570964813232 ,std_cost = 0.013025726191699505 ,avg_grad = 26582.5859375 ,std_grad = 462.2702941894531\n",
      "RBM fitting: 25it [00:00, 146.20it/s]\n",
      "Epoch:14 ,avg_cost = 5.051727771759033 ,std_cost = 0.012432742863893509 ,avg_grad = 26597.982421875 ,std_grad = 361.1313171386719\n",
      "RBM fitting: 25it [00:00, 137.35it/s]\n",
      "Epoch:15 ,avg_cost = 5.05020809173584 ,std_cost = 0.011406796053051949 ,avg_grad = 26630.537109375 ,std_grad = 367.9166259765625\n",
      "RBM fitting: 25it [00:00, 102.53it/s]\n",
      "Epoch:16 ,avg_cost = 5.053430557250977 ,std_cost = 0.013871759176254272 ,avg_grad = 26578.23046875 ,std_grad = 369.5711364746094\n",
      "RBM fitting: 25it [00:00, 126.74it/s]\n",
      "Epoch:17 ,avg_cost = 5.049898624420166 ,std_cost = 0.008375260047614574 ,avg_grad = 26605.974609375 ,std_grad = 386.9801940917969\n",
      "RBM fitting: 25it [00:00, 112.67it/s]\n",
      "Epoch:18 ,avg_cost = 5.051718235015869 ,std_cost = 0.012751101516187191 ,avg_grad = 26689.001953125 ,std_grad = 451.4857482910156\n",
      "RBM fitting: 25it [00:00, 142.90it/s]\n",
      "Epoch:19 ,avg_cost = 5.054787635803223 ,std_cost = 0.010707811452448368 ,avg_grad = 26565.375 ,std_grad = 458.3013916015625\n",
      "RBM fitting: 25it [00:00, 142.16it/s]\n",
      "Epoch:20 ,avg_cost = 5.052127361297607 ,std_cost = 0.014841119758784771 ,avg_grad = 26532.009765625 ,std_grad = 453.99652099609375\n",
      "--------------------\n",
      "Training the 4 st rbm layer\n",
      "RBM fitting: 25it [00:00, 146.11it/s]\n",
      "Epoch:1 ,avg_cost = 4.511679649353027 ,std_cost = 0.15221738815307617 ,avg_grad = 17233.759765625 ,std_grad = 1842.2960205078125\n",
      "RBM fitting: 25it [00:00, 151.22it/s]\n",
      "Epoch:2 ,avg_cost = 4.4232707023620605 ,std_cost = 0.03814435377717018 ,avg_grad = 15549.021484375 ,std_grad = 207.6608428955078\n",
      "RBM fitting: 25it [00:00, 149.88it/s]\n",
      "Epoch:3 ,avg_cost = 4.417047023773193 ,std_cost = 0.03613721579313278 ,avg_grad = 15382.0224609375 ,std_grad = 217.40390014648438\n",
      "RBM fitting: 25it [00:00, 152.16it/s]\n",
      "Epoch:4 ,avg_cost = 4.412248134613037 ,std_cost = 0.03595340996980667 ,avg_grad = 15306.8828125 ,std_grad = 196.99513244628906\n",
      "RBM fitting: 25it [00:00, 151.28it/s]\n",
      "Epoch:5 ,avg_cost = 4.411712646484375 ,std_cost = 0.03626047074794769 ,avg_grad = 15265.0751953125 ,std_grad = 230.32286071777344\n",
      "RBM fitting: 25it [00:00, 149.22it/s]\n",
      "Epoch:6 ,avg_cost = 4.4086432456970215 ,std_cost = 0.03421984985470772 ,avg_grad = 15165.8974609375 ,std_grad = 216.85279846191406\n",
      "RBM fitting: 25it [00:00, 148.71it/s]\n",
      "Epoch:7 ,avg_cost = 4.4088850021362305 ,std_cost = 0.03596174344420433 ,avg_grad = 15194.6845703125 ,std_grad = 243.09519958496094\n",
      "RBM fitting: 25it [00:00, 149.09it/s]\n",
      "Epoch:8 ,avg_cost = 4.408489227294922 ,std_cost = 0.03692176192998886 ,avg_grad = 15195.40625 ,std_grad = 260.0937194824219\n",
      "RBM fitting: 25it [00:00, 150.18it/s]\n",
      "Epoch:9 ,avg_cost = 4.410029888153076 ,std_cost = 0.036086395382881165 ,avg_grad = 15186.5546875 ,std_grad = 220.91885375976562\n",
      "RBM fitting: 25it [00:00, 150.55it/s]\n",
      "Epoch:10 ,avg_cost = 4.4066243171691895 ,std_cost = 0.03698204830288887 ,avg_grad = 15208.3037109375 ,std_grad = 189.5709991455078\n",
      "RBM fitting: 25it [00:00, 148.87it/s]\n",
      "Epoch:11 ,avg_cost = 4.407832145690918 ,std_cost = 0.036771010607481 ,avg_grad = 15150.5634765625 ,std_grad = 216.57015991210938\n",
      "RBM fitting: 25it [00:00, 150.11it/s]\n",
      "Epoch:12 ,avg_cost = 4.404209136962891 ,std_cost = 0.03369696065783501 ,avg_grad = 15197.798828125 ,std_grad = 234.67506408691406\n",
      "RBM fitting: 25it [00:00, 150.12it/s]\n",
      "Epoch:13 ,avg_cost = 4.408033847808838 ,std_cost = 0.038304802030324936 ,avg_grad = 15152.943359375 ,std_grad = 201.85169982910156\n",
      "RBM fitting: 25it [00:00, 144.80it/s]\n",
      "Epoch:14 ,avg_cost = 4.406131267547607 ,std_cost = 0.035280730575323105 ,avg_grad = 15215.5361328125 ,std_grad = 253.96881103515625\n",
      "RBM fitting: 25it [00:00, 149.60it/s]\n",
      "Epoch:15 ,avg_cost = 4.404682159423828 ,std_cost = 0.036226894706487656 ,avg_grad = 15199.388671875 ,std_grad = 205.88192749023438\n",
      "RBM fitting: 25it [00:00, 148.11it/s]\n",
      "Epoch:16 ,avg_cost = 4.4063825607299805 ,std_cost = 0.038107872009277344 ,avg_grad = 15263.8720703125 ,std_grad = 147.53350830078125\n",
      "RBM fitting: 25it [00:00, 149.86it/s]\n",
      "Epoch:17 ,avg_cost = 4.405215263366699 ,std_cost = 0.037883225828409195 ,avg_grad = 15192.701171875 ,std_grad = 229.62318420410156\n",
      "RBM fitting: 25it [00:00, 126.73it/s]\n",
      "Epoch:18 ,avg_cost = 4.4055280685424805 ,std_cost = 0.037319380789995193 ,avg_grad = 15236.54296875 ,std_grad = 211.54881286621094\n",
      "RBM fitting: 25it [00:00, 151.69it/s]\n",
      "Epoch:19 ,avg_cost = 4.405868053436279 ,std_cost = 0.0362672321498394 ,avg_grad = 15234.642578125 ,std_grad = 196.30345153808594\n",
      "RBM fitting: 25it [00:00, 146.93it/s]\n",
      "Epoch:20 ,avg_cost = 4.406298637390137 ,std_cost = 0.0361776165664196 ,avg_grad = 15253.7109375 ,std_grad = 167.68914794921875\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "batch_size = 20\n",
    "\n",
    "dbn_mnist.train_static(inps,outs, num_epochs , batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dbn_mnist(inps[:1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dbn_mnist.state_dict()['W_rec%i'%1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "nn_mlp = DBN_C([num_input, 123*23 ,23*23, 30*23, 18*18]).to(device)  # middle layer\n",
    "\n",
    "# nn_mlp.load_state_dict(dbn_mnist.state_dict(), strict=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0.])\n",
      "tensor([[-0.0241,  0.0143, -0.0006,  ...,  0.0069,  0.0041, -0.0005],\n",
      "        [-0.0216,  0.0283,  0.0095,  ..., -0.0193,  0.0060, -0.0138],\n",
      "        [-0.0231,  0.0157, -0.0046,  ...,  0.0164,  0.0006,  0.0153],\n",
      "        [-0.0025,  0.0032,  0.0163,  ..., -0.0252, -0.0103,  0.0136],\n",
      "        [-0.0020, -0.0211, -0.0075,  ...,  0.0036, -0.0165, -0.0153]])\n",
      "tensor([[-0.0241,  0.0143, -0.0006,  ...,  0.0069,  0.0041, -0.0005],\n",
      "        [-0.0216,  0.0283,  0.0095,  ..., -0.0193,  0.0060, -0.0138],\n",
      "        [-0.0231,  0.0157, -0.0046,  ...,  0.0164,  0.0006,  0.0153],\n",
      "        [-0.0025,  0.0032,  0.0163,  ..., -0.0252, -0.0103,  0.0136],\n",
      "        [-0.0020, -0.0211, -0.0075,  ...,  0.0036, -0.0165, -0.0153]])\n"
     ]
    }
   ],
   "source": [
    "# copy the trained DBN for new NN\n",
    "layers = len(nn_shape)-1\n",
    "nn_mlp = DBN_C([num_input, 23*23 ,23*23, 30*23, 18*18])\n",
    "for i in [1]:\n",
    "    if i == layers-1:   # last layer\n",
    "        nn_mlp.state_dict()['model.%i.weight'%2*i][:] = dbn_mnist.state_dict()['W_mem'][:]\n",
    "        nn_mlp.state_dict()['model.%i.bias'%2*i][:] = dbn_mnist.state_dict()['h_bias_mem'][:]\n",
    "    else:\n",
    "        print(i)\n",
    "        nn_mlp.state_dict()['model.%i.weight'%2*i][:] = dbn_mnist.state_dict()['W_rec%i'%i][:]\n",
    "        nn_mlp.state_dict()['model.%i.bias'%2*i][:] = dbn_mnist.state_dict()['bias_rec%i'%i][:]\n",
    "        print(nn_mlp.state_dict()['model.%i.bias'%2][:5])\n",
    "        print(dbn_mnist.state_dict()['bias_rec%i'%i][:5])\n",
    "i=1\n",
    "# nn_mlp.state_dict()['model.%i.bias'%2*i][:]= dbn_mnist.state_dict()['bias_rec%i'%i][:]\n",
    "print(nn_mlp.state_dict()['model.%i.weight'%2*i][:5])\n",
    "# print(nn_mlp.state_dict()['model.%i.bias'%2*i])\n",
    "print(dbn_mnist.state_dict()['W_rec%i'%i][:5])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "odict_keys(['model.0.weight', 'model.0.bias', 'model.2.weight', 'model.2.bias', 'model.4.weight', 'model.4.bias', 'model.6.weight', 'model.6.bias'])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_mlp.state_dict().keys()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[0.2015, 0.3287, 0.3065, 0.4068, 0.2829, 0.4476, 0.1802, 0.4550, 0.6412,\n          0.4885, 0.4545, 0.7448, 0.6926, 0.4626, 0.7484, 0.7320, 0.6925, 0.3099,\n          0.3314, 0.4540, 0.4238, 0.6577, 0.3262, 0.4054, 0.4135, 0.4294, 0.3872,\n          0.7419, 0.5482, 0.7144, 0.5732, 0.4287, 0.2338, 0.1624, 0.4013, 0.4472,\n          0.5828, 0.5290, 0.3311, 0.3654, 0.4680, 0.3462, 0.4445, 0.5618, 0.3161,\n          0.4289, 0.5324, 0.1796, 0.3753, 0.3440, 0.7034, 0.3720, 0.2386, 0.2060,\n          0.4733, 0.4623, 0.3535, 0.2907, 0.4364, 0.3562, 0.2658, 0.3926, 0.5273,\n          0.5016, 0.7128, 0.7961, 0.2156, 0.5733, 0.4009, 0.3968, 0.4339, 0.3952,\n          0.3055, 0.5650, 0.4733, 0.5012, 0.4413, 0.4182, 0.6628, 0.2006, 0.4323,\n          0.5046, 0.5472, 0.2793, 0.2289, 0.2568, 0.6460, 0.3905, 0.2508, 0.3728,\n          0.3599, 0.2499, 0.3014, 0.5668, 0.6186, 0.2822, 0.4171, 0.2617, 0.5024,\n          0.3024, 0.4374, 0.2079, 0.4935, 0.2748, 0.4304, 0.6037, 0.5785, 0.4493,\n          0.3452, 0.3610, 0.4427, 0.2547, 0.4828, 0.7989, 0.4254, 0.6347, 0.2986,\n          0.4077, 0.5881, 0.4761, 0.2905, 0.5455, 0.4331, 0.5419, 0.5089, 0.4575,\n          0.1610, 0.5826, 0.5703, 0.3567, 0.5926, 0.2284, 0.2773, 0.4924, 0.3342,\n          0.5115, 0.6284, 0.3277, 0.3105, 0.3781, 0.6557, 0.5982, 0.5450, 0.5254,\n          0.4489, 0.3475, 0.3045, 0.6711, 0.6314, 0.4125, 0.4829, 0.5388, 0.1903,\n          0.5616, 0.5503, 0.4409, 0.4801, 0.4273, 0.6079, 0.1710, 0.7163, 0.3329,\n          0.2948, 0.4320, 0.3580, 0.3137, 0.2209, 0.5631, 0.4429, 0.5647, 0.5717,\n          0.2447, 0.4083, 0.4907, 0.5777, 0.2944, 0.2317, 0.5168, 0.7103, 0.7629,\n          0.5793, 0.5114, 0.4555, 0.6612, 0.8035, 0.4124, 0.4152, 0.2076, 0.7537,\n          0.7408, 0.4624, 0.1705, 0.6071, 0.6695, 0.3433, 0.1425, 0.3143, 0.3120,\n          0.2808, 0.4987, 0.3729, 0.2070, 0.2356, 0.2025, 0.1986, 0.4865, 0.3429,\n          0.1982, 0.5967, 0.2390, 0.4946, 0.4595, 0.7356, 0.5805, 0.6665, 0.3172,\n          0.2262, 0.3149, 0.1515, 0.2969, 0.5432, 0.3843, 0.3784, 0.4124, 0.3086,\n          0.2903, 0.4796, 0.3497, 0.2370, 0.3469, 0.2969, 0.3034, 0.5834, 0.5794,\n          0.4543, 0.4222, 0.6156, 0.6076, 0.5376, 0.3612, 0.4408, 0.4201, 0.7700,\n          0.2727, 0.6072, 0.5410, 0.4117, 0.3338, 0.3783, 0.6858, 0.4903, 0.3152,\n          0.5188, 0.4922, 0.3964, 0.5422, 0.4577, 0.6630, 0.5551, 0.5945, 0.2830,\n          0.6577, 0.5326, 0.2898, 0.6021, 0.4642, 0.2547, 0.6462, 0.7970, 0.6960,\n          0.4108, 0.5581, 0.5657, 0.5447, 0.2979, 0.1781, 0.4597, 0.4516, 0.5491,\n          0.5557, 0.2470, 0.0908, 0.2031, 0.6222, 0.1922, 0.4166, 0.5371, 0.1398,\n          0.5702, 0.6974, 0.2587, 0.3028, 0.5832, 0.6095, 0.5278, 0.5097, 0.3384,\n          0.6387, 0.4733, 0.8569, 0.8324, 0.6519, 0.3217, 0.3502, 0.5733, 0.3901,\n          0.1725, 0.2085, 0.6321, 0.2773, 0.5660, 0.3921, 0.5663, 0.3310, 0.2948,\n          0.2500, 0.3686, 0.3001, 0.3898, 0.1318, 0.3681, 0.3585, 0.3835, 0.3961]]),\n tensor([[0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n          0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1.,\n          0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n          0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1.,\n          1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n          0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0.,\n          0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0.,\n          0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1.,\n          1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n          0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0.,\n          1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0.,\n          0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1.,\n          0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0.,\n          1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1.,\n          0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,\n          1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0.,\n          1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1.,\n          0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0.]]))"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbn_mnist(inps[:1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.4867, 0.5242, 0.6130, 0.4757, 0.4726, 0.4273, 0.3214, 0.5480, 0.5057,\n         0.6445, 0.4526, 0.5080, 0.5879, 0.5883, 0.4232, 0.5231, 0.6153, 0.5006,\n         0.5932, 0.4230, 0.5318, 0.5742, 0.4817, 0.4704, 0.4428, 0.4683, 0.6576,\n         0.4905, 0.5205, 0.5115, 0.5447, 0.5071, 0.4491, 0.4875, 0.5651, 0.5039,\n         0.5548, 0.5653, 0.5387, 0.4842, 0.5697, 0.6856, 0.6391, 0.5257, 0.5562,\n         0.5731, 0.4544, 0.3489, 0.5345, 0.5043, 0.5300, 0.5466, 0.5327, 0.5265,\n         0.4890, 0.4465, 0.4256, 0.5068, 0.5466, 0.5067, 0.4360, 0.5251, 0.4883,\n         0.4477, 0.4644, 0.4918, 0.5755, 0.5688, 0.5718, 0.5959, 0.3609, 0.5468,\n         0.6072, 0.5146, 0.6910, 0.4875, 0.4903, 0.5898, 0.4790, 0.4306, 0.4575,\n         0.5434, 0.4116, 0.5068, 0.3955, 0.5275, 0.6195, 0.3692, 0.4907, 0.4699,\n         0.4505, 0.6234, 0.4562, 0.6264, 0.4486, 0.5478, 0.5209, 0.4984, 0.4122,\n         0.5874, 0.4661, 0.4411, 0.5096, 0.4057, 0.4755, 0.4950, 0.5384, 0.4617,\n         0.5173, 0.5396, 0.4418, 0.4606, 0.5471, 0.4597, 0.5859, 0.4810, 0.5963,\n         0.3467, 0.4604, 0.5598, 0.5360, 0.4182, 0.5198, 0.5270, 0.4943, 0.5717,\n         0.4335, 0.5087, 0.4405, 0.4857, 0.5133, 0.5028, 0.4334, 0.4614, 0.5652,\n         0.5119, 0.4150, 0.4125, 0.5481, 0.4876, 0.4467, 0.4433, 0.5022, 0.5587,\n         0.5275, 0.6001, 0.4787, 0.4464, 0.4634, 0.4416, 0.5175, 0.5741, 0.4534,\n         0.6890, 0.5049, 0.5247, 0.4435, 0.5074, 0.5067, 0.4822, 0.5765, 0.5300,\n         0.6339, 0.4986, 0.5480, 0.4146, 0.5382, 0.4314, 0.4808, 0.6204, 0.5940,\n         0.5828, 0.4443, 0.4914, 0.4238, 0.5513, 0.4089, 0.4807, 0.4845, 0.3695,\n         0.4970, 0.6257, 0.5392, 0.6328, 0.4724, 0.4401, 0.4713, 0.6083, 0.4117,\n         0.6015, 0.4581, 0.5870, 0.5915, 0.4773, 0.5656, 0.4484, 0.6155, 0.5456,\n         0.4913, 0.4776, 0.4994, 0.5426, 0.3385, 0.5542, 0.5438, 0.6243, 0.4806,\n         0.5496, 0.5687, 0.6552, 0.5677, 0.4347, 0.6398, 0.4721, 0.4383, 0.5750,\n         0.4692, 0.5261, 0.4685, 0.4513, 0.4379, 0.5445, 0.4137, 0.4461, 0.4545,\n         0.4848, 0.4439, 0.5589, 0.6211, 0.4923, 0.4882, 0.3483, 0.3944, 0.5934,\n         0.6439, 0.5784, 0.3883, 0.4838, 0.5741, 0.4983, 0.4516, 0.6397, 0.5776,\n         0.6493, 0.3511, 0.4251, 0.4964, 0.4755, 0.5436, 0.4858, 0.5314, 0.4490,\n         0.4564, 0.5040, 0.4754, 0.5262, 0.6590, 0.4467, 0.5041, 0.4555, 0.4864,\n         0.6370, 0.4901, 0.3597, 0.5465, 0.4798, 0.4694, 0.4375, 0.6023, 0.3597,\n         0.4561, 0.5588, 0.5166, 0.3989, 0.4650, 0.4612, 0.4677, 0.4372, 0.5353,\n         0.5013, 0.5534, 0.5319, 0.4905, 0.5445, 0.5313, 0.4147, 0.3618, 0.4647,\n         0.5020, 0.5109, 0.3721, 0.3473, 0.3875, 0.6180, 0.6187, 0.4112, 0.3856,\n         0.5017, 0.4274, 0.4631, 0.6494, 0.4713, 0.5720, 0.4619, 0.3780, 0.4999,\n         0.5011, 0.5854, 0.4610, 0.4964, 0.5272, 0.5250, 0.5426, 0.5488, 0.4687,\n         0.2850, 0.5377, 0.5891, 0.5712, 0.4847, 0.3873, 0.4434, 0.5931, 0.4302]],\n       grad_fn=<SigmoidBackward0>)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_mlp(inps[:1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualising layer 1\n",
    "learned_weights = dbn_mnist.rbm_layers[0].W.transpose(0,1).numpy()\n",
    "plt.show()\n",
    "fig = plt.figure(3, figsize=(10,10))\n",
    "for i in range(25): \n",
    "    sub = fig.add_subplot(5, 5, i+1)\n",
    "    sub.imshow(learned_weights[i,:].reshape((28,28)), cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualising layer 2\n",
    "learned_weights = dbn_mnist.rbm_layers[1].W.transpose(0,1).numpy()\n",
    "plt.show()\n",
    "fig = plt.figure(3, figsize=(10,10))\n",
    "for i in range(25): \n",
    "    sub = fig.add_subplot(5, 5, i+1)\n",
    "    sub.imshow(learned_weights[i,:].reshape((23,23)), cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number = 5 #A number between 0 and 10.\n",
    "\n",
    "particular_mnist = []\n",
    "\n",
    "limit = mnist_data.data.shape[0]\n",
    "# limit = 60000\n",
    "for i in range(limit):\n",
    "    if(mnist_data.targets[i] == number):\n",
    "        particular_mnist.append(mnist_data.data[i].numpy())\n",
    "# particular_mnist = np.array(particular_mnist)\n",
    "len(particular_mnist)\n",
    "# mnist_data.train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.stack([torch.Tensor(i) for i in particular_mnist])\n",
    "train_label = torch.stack([torch.Tensor(number) for i in range(len(particular_mnist))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dbn_mnist.train_static(train_data,train_label,30 , batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 3\n",
    "img = mnist_data.train_data[idx]\n",
    "reconstructed_img = img.view(1,-1).type(torch.FloatTensor)\n",
    "\n",
    "_,reconstructed_img= dbn_mnist.reconstruct(reconstructed_img)\n",
    "\n",
    "reconstructed_img = reconstructed_img.view((28,28))\n",
    "print(\"The original number: {}\".format(mnist_data.train_labels[idx]))\n",
    "plt.imshow(img , cmap = 'gray')\n",
    "plt.show()\n",
    "print(\"The reconstructed image\")\n",
    "plt.imshow(reconstructed_img , cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
